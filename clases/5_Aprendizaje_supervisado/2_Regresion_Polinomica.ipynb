{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "DHNQuIqAmvl5",
      "metadata": {
        "id": "DHNQuIqAmvl5"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LinaMariaCastro/curso-ia-para-economia/blob/main/clases/5_Aprendizaje_supervisado/2_Regresion_Polinomica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5iiwfbXCQv5D",
      "metadata": {
        "id": "5iiwfbXCQv5D"
      },
      "source": [
        "# **Inteligencia Artificial con Aplicaciones en Econom√≠a I**\n",
        "\n",
        "- üë©‚Äçüè´ **Profesora:** [Lina Mar√≠a Castro](https://www.linkedin.com/in/lina-maria-castro)  \n",
        "- üìß **Email:** [lmcastroco@gmail.com](mailto:lmcastroco@gmail.com)  \n",
        "- üéì **Universidad:** Universidad Externado de Colombia - Facultad de Econom√≠a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7TpqpFM9PQbp",
      "metadata": {
        "id": "7TpqpFM9PQbp"
      },
      "source": [
        "# ‚öñÔ∏è **Regresi√≥n Polin√≥mica, Subajuste, Sobreajuste y Regularizaci√≥n**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F1l5E-8fEA2a",
      "metadata": {
        "id": "F1l5E-8fEA2a"
      },
      "source": [
        "**Objetivos de Aprendizaje**\n",
        "\n",
        "Al finalizar este notebook, los estudiantes ser√°s capaz de:\n",
        "\n",
        "1. **Entrenar modelos de Regresi√≥n Polin√≥mica.**\n",
        "2.  **Entender los problemas de `underfitting` y `overfitting`** y el Trade-off entre Sesgo y Varianza.\n",
        "3.  **Implementar la Regularizaci√≥n (Ridge y Lasso)** como una t√©cnica fundamental para controlar la complejidad del modelo y mejorar su capacidad de generalizaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3SVOGrnBU-nZ",
      "metadata": {
        "id": "3SVOGrnBU-nZ"
      },
      "source": [
        "## Importar Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2_8XcDZJEA2r",
      "metadata": {
        "id": "2_8XcDZJEA2r"
      },
      "outputs": [],
      "source": [
        "# Importaci√≥n de librer√≠as est√°ndar\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Importaci√≥n de herramientas de scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jHE2ZLJuEPIe",
      "metadata": {
        "id": "jHE2ZLJuEPIe"
      },
      "source": [
        "## Mejorar visualizaci√≥n de dataframes y gr√°ficos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72TA8V1fETCm",
      "metadata": {
        "id": "72TA8V1fETCm"
      },
      "outputs": [],
      "source": [
        "# Que muestre todas las columnas\n",
        "pd.options.display.max_columns = None\n",
        "# En los dataframes, mostrar los float con dos decimales\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "\n",
        "# Configuraciones para una mejor visualizaci√≥n\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dp4z8RCiVVjq",
      "metadata": {
        "id": "Dp4z8RCiVVjq"
      },
      "source": [
        "## Cargar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KYRaLbXlMc_c",
      "metadata": {
        "id": "KYRaLbXlMc_c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dQSa_neuGLji",
      "metadata": {
        "id": "dQSa_neuGLji"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/2025_ii_curso_ia_economia/datasets'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cka8EDXhQuee",
      "metadata": {
        "id": "cka8EDXhQuee"
      },
      "outputs": [],
      "source": [
        "# Para establecer el directorio de los archivos\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T7Vj-X81VfZg",
      "metadata": {
        "id": "T7Vj-X81VfZg"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Salary_Data.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NJnsbQZVEA2t",
      "metadata": {
        "id": "NJnsbQZVEA2t"
      },
      "source": [
        "## Limpieza de Datos\n",
        "\n",
        "Inspeccionamos los tipos de datos y los valores nulos, tal como se har√≠a en un proyecto real (y como se hace en el notebook de referencia)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "awzj_rASV1y6",
      "metadata": {
        "id": "awzj_rASV1y6"
      },
      "outputs": [],
      "source": [
        "print(\"Informaci√≥n del DataFrame:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ht3Vf2iEA2u",
      "metadata": {
        "id": "9ht3Vf2iEA2u"
      },
      "outputs": [],
      "source": [
        "print(\"\\nValores Nulos por Columna:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VHcnDC2rWC2M",
      "metadata": {
        "id": "VHcnDC2rWC2M"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum()/len(df)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3wq06K_BEA2y",
      "metadata": {
        "id": "3wq06K_BEA2y"
      },
      "source": [
        "Vemos que hay los valores nulos son muy pocos, por lo que vamos a eliminarlos, ya que esto no afectar√° significativamente al modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4h72TX5cEA2y",
      "metadata": {
        "id": "4h72TX5cEA2y"
      },
      "outputs": [],
      "source": [
        "# Eliminamos las filas con cualquier valor nulo\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(\"Valores Nulos despu√©s de la limpieza:\")\n",
        "print(df.isnull().sum().sum()) # .sum().sum() da el total de nulos en todo el DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fTatI5yuEA20",
      "metadata": {
        "id": "fTatI5yuEA20"
      },
      "source": [
        "## Enfoque Pedag√≥gico\n",
        "\n",
        "El dataset tiene m√∫ltiples variables (categ√≥ricas y num√©ricas). Un modelo completo requerir√≠a *One-Hot Encoding* o *Label Encoding* para `Gender`, `Education Level` y `Job Title`.\n",
        "\n",
        "**Sin embargo, para el objetivo de *esta clase* (entender el sobreajuste visualmente),** hacer todo ese preprocesamiento introduce demasiada complejidad que distrae del punto principal.\n",
        "\n",
        "Nos enfocaremos en la relaci√≥n econ√≥mica m√°s cl√°sica: **Teor√≠a del Capital Humano**, que postula que el salario (`Salary`) es una funci√≥n de la experiencia (`Years of Experience`). Esto nos permite modelar y visualizar el problema en 2D de forma muy clara."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4YaHEBJ5EA20",
      "metadata": {
        "id": "4YaHEBJ5EA20"
      },
      "source": [
        "## Divisi√≥n del Conjunto de Datos\n",
        "\n",
        "Primero, seleccionamos nuestras variables `X` e `y`. Luego, dividimos los datos. **El EDA se debe hacer *despu√©s* de la divisi√≥n y *solo* sobre los datos de entrenamiento**, para evitar contaminar nuestro an√°lisis con informaci√≥n del conjunto de prueba (lo que se conoce como *data leakage* o fuga de datos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wYvwt_W0EA21",
      "metadata": {
        "id": "wYvwt_W0EA21"
      },
      "outputs": [],
      "source": [
        "# 1. Seleccionar las variables de inter√©s\n",
        "X = df[['Years of Experience']]\n",
        "y = df['Salary']\n",
        "\n",
        "# 2. Dividir en conjuntos de entrenamiento y prueba (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Tama√±o del set de Entrenamiento: {X_train.shape[0]} observaciones\")\n",
        "print(f\"Tama√±o del set de Prueba: {X_test.shape[0]} observaciones\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49jOj0kQYK_p",
      "metadata": {
        "id": "49jOj0kQYK_p"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H6Gm1U5xYOCM",
      "metadata": {
        "id": "H6Gm1U5xYOCM"
      },
      "outputs": [],
      "source": [
        "y_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JOuRbE_7EA21",
      "metadata": {
        "id": "JOuRbE_7EA21"
      },
      "source": [
        "## **An√°lisis Exploratorio de Datos (EDA) sobre Datos de Entrenamiento**\n",
        "\n",
        "Ahora, exploremos *√∫nicamente* los datos de `(X_train, y_train)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oESkjtkZEA21",
      "metadata": {
        "id": "oESkjtkZEA21"
      },
      "outputs": [],
      "source": [
        "# 1. Relaci√≥n entre Experiencia y Salario\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_train['Years of Experience'], y=y_train, alpha=0.6)\n",
        "plt.title('EDA: Relaci√≥n entre A√±os de Experiencia y Salario (Datos de Entrenamiento)')\n",
        "plt.xlabel('A√±os de Experiencia')\n",
        "plt.ylabel('Salario (USD)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LwmLLjDvEA21",
      "metadata": {
        "id": "LwmLLjDvEA21"
      },
      "source": [
        "**Interpretaci√≥n del EDA:**\n",
        "\n",
        "Observamos una relaci√≥n positiva muy fuerte: a m√°s experiencia, mayor salario. Sin embargo, la nube de puntos parece curvarse ligeramente a medida que aumenta la experiencia. Esto sugiere que un modelo lineal simple (una sola recta) no es adecuado. La teor√≠a econ√≥mica (rendimientos marginales de la experiencia) apoyar√≠a la idea de que la relaci√≥n no es perfectamente lineal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9SHBW_dEA22",
      "metadata": {
        "id": "e9SHBW_dEA22"
      },
      "outputs": [],
      "source": [
        "# 2. Distribuci√≥n de las variables\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "sns.histplot(y_train, kde=True, ax=axes[0], bins=30)\n",
        "axes[0].set_title('Distribuci√≥n de Salarios (Entrenamiento)')\n",
        "axes[0].set_xlabel('Salario (USD)')\n",
        "\n",
        "sns.histplot(X_train['Years of Experience'], kde=True, ax=axes[1], bins=20)\n",
        "axes[1].set_title('Distribuci√≥n de A√±os de Experiencia (Entrenamiento)')\n",
        "axes[1].set_xlabel('A√±os de Experiencia')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BUuv1dwUEA22",
      "metadata": {
        "id": "BUuv1dwUEA22"
      },
      "source": [
        "## Modelo 1 - Subajuste (Underfitting)\n",
        "\n",
        "Comencemos con el modelo m√°s simple posible: una regresi√≥n lineal simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ZhycTX1EA23",
      "metadata": {
        "id": "5ZhycTX1EA23"
      },
      "outputs": [],
      "source": [
        "# Creaci√≥n y entrenamiento del modelo lineal simple\n",
        "model_simple = LinearRegression()\n",
        "model_simple.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones\n",
        "y_train_pred_simple = model_simple.predict(X_train)\n",
        "y_test_pred_simple = model_simple.predict(X_test)\n",
        "\n",
        "# C√°lculo del error (RMSE - Root Mean Squared Error)\n",
        "# Usamos RMSE para que el error est√© en las mismas unidades que el salario (USD)\n",
        "rmse_train_simple = np.sqrt(mean_squared_error(y_train, y_train_pred_simple))\n",
        "rmse_test_simple = np.sqrt(mean_squared_error(y_test, y_test_pred_simple))\n",
        "\n",
        "print(f\"RMSE en Entrenamiento (Lineal): ${rmse_train_simple:,.2f}\")\n",
        "print(f\"RMSE en Prueba (Lineal): ${rmse_test_simple:,.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ByNYVhLkEA23",
      "metadata": {
        "id": "ByNYVhLkEA23"
      },
      "outputs": [],
      "source": [
        "# Visualizaci√≥n del modelo de Subajuste\n",
        "\n",
        "# Dibujar una l√≠nea de regresi√≥n suave y continua\n",
        "# Para la l√≠nea, creamos puntos ordenados de X\n",
        "X_plot = np.linspace(X_train.min(), X_train.max(), 100)\n",
        "y_plot_simple = model_simple.predict(X_plot)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train, y_train, alpha=0.3, label='Datos de Entrenamiento')\n",
        "plt.plot(X_plot, y_plot_simple, color='red', linewidth=3, label='Regresi√≥n Lineal Simple (Alto Sesgo)')\n",
        "plt.title('Modelo 1: Subajuste (Underfitting)')\n",
        "plt.xlabel('A√±os de Experiencia')\n",
        "plt.ylabel('Salario (USD)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_d8UKmUgEA23",
      "metadata": {
        "id": "_d8UKmUgEA23"
      },
      "source": [
        "**Interpretaci√≥n:**\n",
        "\n",
        "El modelo lineal captura la tendencia general, pero podemos ver visualmente que **sobrestima** sistem√°ticamente los salarios para empleados con m√°s de 20 a√±os de experiencia."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XwGM8eEAEA24",
      "metadata": {
        "id": "XwGM8eEAEA24"
      },
      "source": [
        "## Modelo 2 - Sobreajuste (Overfitting)\n",
        "\n",
        "Ahora, vamos al extremo opuesto. Crearemos un modelo extremadamente flexible (un polinomio de grado 25) que \"memorizar√°\" los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6BqfbcapEA24",
      "metadata": {
        "id": "6BqfbcapEA24"
      },
      "outputs": [],
      "source": [
        "# Creamos un pipeline para automatizar los pasos:\n",
        "# 1. Crear caracter√≠sticas polin√≥micas de grado 25\n",
        "# 2. Escalar las caracter√≠sticas (Explicado arriba)\n",
        "# 3. Ajustar un modelo lineal a esas caracter√≠sticas\n",
        "\n",
        "pipeline_poly = Pipeline([\n",
        "    (\"poly_features\", PolynomialFeatures(degree=25, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LinearRegression())\n",
        "])\n",
        "\n",
        "pipeline_poly.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones y error\n",
        "y_train_pred_poly = pipeline_poly.predict(X_train)\n",
        "y_test_pred_poly = pipeline_poly.predict(X_test)\n",
        "\n",
        "rmse_train_poly = np.sqrt(mean_squared_error(y_train, y_train_pred_poly))\n",
        "rmse_test_poly = np.sqrt(mean_squared_error(y_test, y_test_pred_poly))\n",
        "\n",
        "print(f\"RMSE en Entrenamiento (Poli-25): ${rmse_train_poly:,.2f} <-- ¬°El error de entrenamiento baj√≥!\")\n",
        "print(f\"RMSE en Prueba (Poli-25): ${rmse_test_poly:,.2f} <-- ¬°¬°El error de prueba aument√≥ considerablemente!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9WpGHcuiEA24",
      "metadata": {
        "id": "9WpGHcuiEA24"
      },
      "outputs": [],
      "source": [
        "# Visualizaci√≥n del modelo de Sobreajuste\n",
        "\n",
        "# Para visualizar la curva, usamos los puntos X ordenados\n",
        "y_plot_poly = pipeline_poly.predict(X_plot)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train, y_train, alpha=0.3, label='Datos de Entrenamiento')\n",
        "plt.plot(X_plot, y_plot_poly, color='red', linewidth=3, label='Poli-25 (Alta Varianza)')\n",
        "plt.title('Modelo 2: Sobreajuste (Overfitting)')\n",
        "plt.xlabel('A√±os de Experiencia')\n",
        "plt.ylabel('Salario (USD)')\n",
        "plt.legend()\n",
        "plt.ylim(0, 300000) # Establecemos un l√≠mite en el eje Y\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9bXkp6OEA25",
      "metadata": {
        "id": "b9bXkp6OEA25"
      },
      "source": [
        "**Interpretaci√≥n:**\n",
        "\n",
        "¬°Miren el desastre! La curva se vuelve loca, tratando de pasar por cada punto. El RMSE de entrenamiento es m√°s bajo que el del modelo lineal (~$27.000), lo que indica que es \"mejor\" para describir los datos que ya vio.\n",
        "\n",
        "Sin embargo, la curva se vuelve **err√°tica** y al enfrentarse a los datos de prueba, el error aumenta bastante (~$135.000). Este modelo **memoriz√≥ el ruido**, no el patr√≥n subyacente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tR-vmB6OEA25",
      "metadata": {
        "id": "tR-vmB6OEA25"
      },
      "source": [
        "## La Soluci√≥n - Regularizaci√≥n (Ridge y Lasso)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T1cKSxpuvemF",
      "metadata": {
        "id": "T1cKSxpuvemF"
      },
      "source": [
        "La regularizaci√≥n es una t√©cnica que penaliza al modelo por ser demasiado complejo. Funciona modificando la \"funci√≥n de costos\" del modelo (la f√≥rmula que intenta minimizar).\n",
        "\n",
        "Regresi√≥n Lineal (OLS): Su objetivo es simple, encontrar los coeficientes ($\\beta$) que minimicen el Error Cuadr√°tico Medio (MSE).\n",
        "\n",
        "Costo = Error\n",
        "\n",
        "Regresi√≥n Lineal Regularizada: Su objetivo es un balance. Quiere minimizar el error, pero tambi√©n quiere minimizar la complejidad. Para ello, le a√±adimos un \"impuesto a la complejidad\".\n",
        "\n",
        "Costo = Error + Penalizaci√≥n\n",
        "\n",
        "El modelo ahora tiene que justificar cada coeficiente. Un coeficiente solo puede ser grande si logra una reducci√≥n importante en el MSE que \"pague\" el costo de su propia penalizaci√≥n.\n",
        "\n",
        "Hay dos tipos principales de penalizaci√≥n que dan lugar a dos modelos: Ridge (L2) y Lasso (L1).\n",
        "\n",
        "1. **Regresi√≥n Ridge (Penalizaci√≥n L2)**\n",
        "\n",
        "La Regresi√≥n Ridge suaviza los coeficientes ($\\beta$).\n",
        "\n",
        "- *C√≥mo funciona:*\n",
        "\n",
        "A√±ade una penalizaci√≥n proporcional al cuadrado de la magnitud de cada coeficiente.\n",
        "\n",
        "Penalizaci√≥n Ridge (L2) = Œ± * Œ£(Œ≤¬≤)\n",
        "\n",
        "- *El Resultado:*\n",
        "\n",
        "La penalizaci√≥n al cuadrado castiga de forma exponencial los coeficientes muy grandes. Si un coeficiente ($\\beta$) intenta crecer de 5 a 10:El error (MSE) puede bajar un poco, pero su penalizaci√≥n sube de $5^2=25$ a $10^2=100$.\n",
        "\n",
        "El modelo r√°pidamente aprende que \"no vale la pena\" tener coeficientes tan grandes y los encoge a todos para encontrar un balance.\n",
        "\n",
        "- *Efecto Clave:*\n",
        "\n",
        "Ridge hace que los coeficientes sean peque√±os. Simplemente los suaviza, \"domando\" la curva err√°tica que vimos. Es √∫til si hay problemas de multicolinealidad, puesto que Ridge tiende a encoger los coeficientes de todas las variables correlacionadas de manera similar, repartiendo su efecto.\n",
        "\n",
        "2. **Regresi√≥n Lasso (Penalizaci√≥n L1)**\n",
        "\n",
        "Lasso es un \"selector de variables\".\n",
        "\n",
        "- *C√≥mo funciona:*\n",
        "\n",
        "A√±ade una penalizaci√≥n proporcional al valor absoluto de cada coeficiente.\n",
        "\n",
        "Penalizaci√≥n Lasso (L1) = Œ± * Œ£(|Œ≤|)\n",
        "\n",
        "- *El Resultado:*\n",
        "\n",
        "La penalizaci√≥n por el valor absoluto tiene una propiedad matem√°tica asombrosa. A medida que aumenta la fuerza de la penalizaci√≥n (alpha), el modelo descubre que la mejor manera de minimizar el costo total es eliminar por completo los coeficientes de las variables menos importantes, llev√°ndolos exactamente a cero.\n",
        "\n",
        "- *Efecto Clave:*\n",
        "\n",
        "Lasso realiza una selecci√≥n autom√°tica de variables. Le das 25 caracter√≠sticas polin√≥micas (ruidosas) y te dice: \"He analizado todas, y la verdad es que 20 de ellas son in√∫tiles (cero), as√≠ que me quedo solo con las 5 que realmente importan\".\n",
        "\n",
        "Algoritmos donde se utilizan Ridge (L2) y Lasso (L1):\n",
        "- Regresi√≥n lineal\n",
        "- Regresi√≥n polin√≥mica\n",
        "- Regresi√≥n Log√≠stica\n",
        "- Redes neuronales\n",
        "- M√°quinas de Soporte Vectorial (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WpBoPWd60Q5H",
      "metadata": {
        "id": "WpBoPWd60Q5H"
      },
      "source": [
        "**¬øC√≥mo determino el valor de alpha al aplicar Ridge y Lasso?**\n",
        "\n",
        "No hay una regla que seguir, ni se debe adivinar. Para encontrar el alpha √≥ptimo se usan t√©cnicas como la Validaci√≥n Cruzada (Cross-Validation), que veremos m√°s adelante, para probar un rango de valores (ej. 0.1, 1, 10, 100, 1000) y encontrar el alpha que da el menor error en datos de prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "usQb0adjEA24",
      "metadata": {
        "id": "usQb0adjEA24"
      },
      "source": [
        "**¬øPor qu√© Estandarizar (`StandardScaler`) al utilizar Ridge y Lasso?**\n",
        "\n",
        "Al utilizar `PolynomialFeatures(degree=25)` se crean nuevas variables: `Experiencia^2`, `Experiencia^3`, `Experiencia^4`, ... `Experiencia^25`.\n",
        "\n",
        "Pensemos en un empleado con 10 a√±os de experiencia:\n",
        "* `Experiencia` = 10\n",
        "* `Experiencia^2` = 100\n",
        "* `Experiencia^15` = 10,000,000,000,000,000,000,000,000\n",
        "\n",
        "Las escalas de nuestras variables son **dr√°sticamente diferentes**. Un modelo de regresi√≥n (incluyendo Ridge y Lasso) penaliza la **magnitud de los coeficientes**.\n",
        "\n",
        "Si no estandarizamos, el modelo penalizar√° injustamente a las variables de menor grado (como `Experiencia`) y tendr√° problemas num√©ricos con las de grado alto.\n",
        "\n",
        "**Recomendaci√≥n: Siempre que uses regularizaci√≥n o caracter√≠sticas polin√≥micas, DEBES estandarizar (escalar) tus datos primero.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TkF1awDevsEE",
      "metadata": {
        "id": "TkF1awDevsEE"
      },
      "source": [
        "Ahora vamos a \"curar\" el sobreajuste. Usaremos el mismo modelo polin√≥mico de grado 25, pero cambiaremos `LinearRegression` por `Ridge` y `Lasso`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bhNzZlRwEA25",
      "metadata": {
        "id": "bhNzZlRwEA25"
      },
      "source": [
        "## Modelo 3: Regresi√≥n Ridge (Penalizaci√≥n L2)\n",
        "\n",
        "Ridge (L2) a√±ade una penalizaci√≥n por los coeficientes *al cuadrado*. \"Encoge\" los coeficientes de las variables ruidosas, pero no los lleva a cero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wvRpwS03EA26",
      "metadata": {
        "id": "wvRpwS03EA26"
      },
      "outputs": [],
      "source": [
        "# Creamos un pipeline con Regresi√≥n Ridge\n",
        "# alpha es la fuerza de la penalizaci√≥n. Empecemos con un valor de 10.\n",
        "\n",
        "pipeline_ridge = Pipeline([\n",
        "    (\"poly_features\", PolynomialFeatures(degree=25, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", Ridge(alpha=10))\n",
        "])\n",
        "\n",
        "pipeline_ridge.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones y error\n",
        "y_train_pred_ridge = pipeline_ridge.predict(X_train)\n",
        "y_test_pred_ridge = pipeline_ridge.predict(X_test)\n",
        "\n",
        "rmse_train_ridge = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge))\n",
        "rmse_test_ridge = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))\n",
        "\n",
        "print(f\"RMSE Entrenamiento (Ridge): ${rmse_train_ridge:,.2f}\")\n",
        "print(f\"RMSE Prueba (Poli-25): ${rmse_test_poly:,.2f} (El modelo sobreajustado)\")\n",
        "print(f\"RMSE Prueba (Ridge): ${rmse_test_ridge:,.2f} <-- ¬°¬°Gran mejora!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L98I-_8mEA26",
      "metadata": {
        "id": "L98I-_8mEA26"
      },
      "outputs": [],
      "source": [
        "# Visualizaci√≥n del modelo Ridge\n",
        "y_plot_ridge = pipeline_ridge.predict(X_plot)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train, y_train, alpha=0.3, label='Datos de Entrenamiento')\n",
        "plt.plot(X_plot, y_plot_poly, color='gray', linestyle='--', label='Poli-25 (Overfit)')\n",
        "plt.plot(X_plot, y_plot_ridge, color='green', linewidth=3, label='Ridge (Regularizado L2)')\n",
        "plt.title('Modelo 3: Curando el Sobreajuste con Ridge')\n",
        "plt.xlabel('A√±os de Experiencia')\n",
        "plt.ylabel('Salario (USD)')\n",
        "plt.legend()\n",
        "plt.ylim(0, 300000)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N5m7lSMoEA26",
      "metadata": {
        "id": "N5m7lSMoEA26"
      },
      "source": [
        "**Interpretaci√≥n:** ¬°Miren la diferencia! La curva de Ridge (verde) es **flexible** (captura la no linealidad mejor que la recta) pero **suave** y **cre√≠ble**.\n",
        "\n",
        "La penalizaci√≥n L2 \"dom√≥\" la complejidad. El RMSE de entrenamiento subi√≥ un poco (lo cual es bueno, dej√≥ de memorizar), pero el RMSE de prueba **baj√≥ dr√°sticamente**. El modelo ahora generaliza."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IPnoV0pSEA26",
      "metadata": {
        "id": "IPnoV0pSEA26"
      },
      "source": [
        "## Modelo 4: Regresi√≥n Lasso (Penalizaci√≥n L1) y Selecci√≥n de Variables\n",
        "\n",
        "Lasso (L1) a√±ade una penalizaci√≥n por el *valor absoluto* de los coeficientes. Su superpoder es que puede llevar coeficientes irrelevantes **exactamente a cero**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U0Px0qblEA26",
      "metadata": {
        "id": "U0Px0qblEA26"
      },
      "outputs": [],
      "source": [
        "# Creamos un pipeline con Regresi√≥n Lasso\n",
        "# Los salarios son n√∫meros grandes, por lo que el error es grande. Necesitamos un alpha grande.\n",
        "# Probemos con alpha=500. Aumentamos max_iter para asegurar que converja.\n",
        "\n",
        "pipeline_lasso = Pipeline([\n",
        "    (\"poly_features\", PolynomialFeatures(degree=25, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", Lasso(alpha=500, max_iter=10000))\n",
        "])\n",
        "\n",
        "pipeline_lasso.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones y error\n",
        "y_train_pred_lasso = pipeline_lasso.predict(X_train)\n",
        "y_test_pred_lasso = pipeline_lasso.predict(X_test)\n",
        "\n",
        "rmse_train_lasso = np.sqrt(mean_squared_error(y_train, y_train_pred_lasso))\n",
        "rmse_test_lasso = np.sqrt(mean_squared_error(y_test, y_test_pred_lasso))\n",
        "\n",
        "print(f\"RMSE Entrenamiento (Lasso): ${rmse_train_lasso:,.2f}\")\n",
        "print(f\"RMSE Prueba (Ridge): ${rmse_test_ridge:,.2f} <-- ¬°El mejor resultado hasta ahora!\")\n",
        "print(f\"RMSE Prueba (Lasso): ${rmse_test_lasso:,.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sU52jZZjEA27",
      "metadata": {
        "id": "sU52jZZjEA27"
      },
      "outputs": [],
      "source": [
        "# Visualizaci√≥n del modelo Lasso\n",
        "y_plot_lasso = pipeline_lasso.predict(X_plot)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train, y_train, alpha=0.3, label='Datos de Entrenamiento')\n",
        "plt.plot(X_plot, y_plot_ridge, color='gray', linestyle='--', label='Ridge')\n",
        "plt.plot(X_plot, y_plot_lasso, color='purple', linewidth=3, label='Lasso (Regularizado L1)')\n",
        "plt.title('Modelo 4: Simplificando con Lasso')\n",
        "plt.xlabel('A√±os de Experiencia')\n",
        "plt.ylabel('Salario (USD)')\n",
        "plt.legend()\n",
        "plt.ylim(0, 300000)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fcc3PCUEA27",
      "metadata": {
        "id": "4fcc3PCUEA27"
      },
      "source": [
        "### La Magia de Lasso: Selecci√≥n de Variables\n",
        "\n",
        "Veamos los coeficientes que Lasso ha aprendido. Recuerden que partimos de un modelo con 25 caracter√≠sticas (`Exp^1`, `Exp^2`, ..., `Exp^25`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ydFau41VEA3M",
      "metadata": {
        "id": "ydFau41VEA3M"
      },
      "outputs": [],
      "source": [
        "# Extraemos los coeficientes del modelo Lasso en el pipeline\n",
        "coefs_lasso = pipeline_lasso.named_steps['model'].coef_\n",
        "\n",
        "num_total_coeffs = len(coefs_lasso)\n",
        "num_zero_coeffs = np.sum(coefs_lasso == 0)\n",
        "\n",
        "print(f\"N√∫mero total de coeficientes (caracter√≠sticas polin√≥micas): {num_total_coeffs}\")\n",
        "print(f\"N√∫mero de coeficientes llevados a CERO por Lasso: {num_zero_coeffs}\")\n",
        "print(f\"N√∫mero de coeficientes CONSERVADOS: {num_total_coeffs - num_zero_coeffs}\")\n",
        "\n",
        "print(\"\\n--- Valor de los Coeficientes ---\")\n",
        "print(coefs_lasso)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z9VOrn-jEA3M",
      "metadata": {
        "id": "Z9VOrn-jEA3M"
      },
      "source": [
        "**Interpretaci√≥n:**\n",
        "\n",
        "¬°Este es el resultado clave! De las 25 caracter√≠sticas polin√≥micas que le dimos, Lasso decidi√≥ que muchas (ej. 20 de 25) eran \"ruido\" y las **elimin√≥ por completo** (llevando sus coeficientes a cero).\n",
        "\n",
        "Actu√≥ como un **m√©todo autom√°tico de selecci√≥n de variables**, d√°ndonos un modelo m√°s simple e interpretable (m√°s **parsimonioso**) y con mejor rendimiento predictivo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dVim3B9fEA3N",
      "metadata": {
        "id": "dVim3B9fEA3N"
      },
      "source": [
        "## Conclusiones y Resumen Comparativo\n",
        "\n",
        "Pongamos todos nuestros resultados en una tabla para ver la historia completa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ITrkx7q7EA3N",
      "metadata": {
        "id": "ITrkx7q7EA3N"
      },
      "outputs": [],
      "source": [
        "# Creando un DataFrame para comparar los errores (RMSE)\n",
        "results = pd.DataFrame({\n",
        "    'Modelo': [\n",
        "        '1. Lineal Simple (Subajuste)',\n",
        "        '2. Polin√≥mico Grado 25 (Sobreajuste)',\n",
        "        '3. Ridge (Regularizado L2)',\n",
        "        '4. Lasso (Regularizado L1)'\n",
        "    ],\n",
        "    'RMSE en Entrenamiento': [\n",
        "        rmse_train_simple,\n",
        "        rmse_train_poly,\n",
        "        rmse_train_ridge,\n",
        "        rmse_train_lasso\n",
        "    ],\n",
        "    'RMSE en Prueba': [\n",
        "        rmse_test_simple,\n",
        "        rmse_test_poly,\n",
        "        rmse_test_ridge,\n",
        "        rmse_test_lasso\n",
        "    ]\n",
        "})\n",
        "\n",
        "results_sorted = results.sort_values(by='RMSE en Prueba')\n",
        "\n",
        "print(\"Tabla Comparativa de Rendimiento (Error en D√≥lares)\")\n",
        "display(results_sorted.style.format({\n",
        "    'RMSE en Entrenamiento': '${:,.2f}',\n",
        "    'RMSE en Prueba': '${:,.2f}'\n",
        "}).background_gradient(cmap='Reds', subset=['RMSE en Prueba']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "az3C6RqTEA3N",
      "metadata": {
        "id": "az3C6RqTEA3N"
      },
      "source": [
        "1.  **Subajuste (Modelo 1):** Es demasiado simple. Falla en capturar el patr√≥n y tiene un error mediocre tanto en entrenamiento como en prueba (RMSE ~$31.000).\n",
        "2.  **Sobreajuste (Modelo 2):** Es demasiado complejo. \"Memoriza\" el ruido de los datos de entrenamiento (RMSE de entrenamiento bajo) pero es in√∫til para predecir nuevos datos (RMSE de prueba explota).\n",
        "3.  **Regularizaci√≥n (Modelos 3 y 4):** Son el \"punto √≥ptimo\". Permiten usar un modelo flexible (polin√≥mico) pero controlan su complejidad, previniendo el sobreajuste.\n",
        "\n",
        "El modelo **Ridge** fue el m√°s preciso para predecir salarios para nuevas observaciones (el menor RMSE de prueba), por tanto, es el modelo que deber√≠amos elegir."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fGZz4URdtK5G",
      "metadata": {
        "id": "fGZz4URdtK5G"
      },
      "source": [
        "**Conclusi√≥n Final:**\n",
        "\n",
        "Partimos de un modelo demasiado simple (subajuste) y luego creamos uno deliberadamente complejo que se sobreajust√≥ al ruido. La regularizaci√≥n, tanto Ridge como Lasso, nos permiti√≥ \"curar\" este sobreajuste, controlando la complejidad y mejorando dr√°sticamente la capacidad del modelo para generalizar a nuevos datos. Lasso, en particular, nos dio el beneficio adicional de simplificar el modelo al realizar una selecci√≥n autom√°tica de variables.\n",
        "\n",
        "El dilema entre overfitting y underfitting es el n√∫cleo del machine learning aplicado. **Nuestro trabajo no es minimizar el error en los datos que ya tenemos, sino construir modelos que sean robustos y fiables para el futuro.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
